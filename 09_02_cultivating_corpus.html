
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2. Cultivating a Corpus &#8212; Introduction to Named Entity Recognition</title>
    
  <link rel="stylesheet" href="_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="3. The Challenges of Holocaust NER" href="10_the_challenges_of_holocaust_ner.html" />
    <link rel="prev" title="1. Cultivating Good Datasets for Entities" href="09_01_cultivating_concentration_camp_dataset.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/data_science_lab_logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Introduction to Named Entity Recognition</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   <p align="center">
    INTRODUCTION TO NAMED ENTITY RECOGNITION
   </p>
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Key Concepts and Terms
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="01_01_introduction_to_ner.html">
   1. Introduction to Named Entity Recognition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="01_02_introduction_to_spacy.html">
   2. Introduction to spaCy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_rules_based_ner.html">
   3. Rules-Based NER with spaCy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_machine_learning_ner.html">
   4. Machine Learning NER with spaCy
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  The Basics of NER Training
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="04_01_spaCy_Entity_Ruler.html">
   1. Using SpaCy's EntityRuler
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_02_create_ner_training_set.html">
   2. Using EntityRuler to Create Training Set
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_03_train_spacy_ner_model.html">
   3. How to Train spaCy NER Model
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Advanced NER Concepts
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="05_examining_a_spacy_model.html">
   1. Examining a spaCy Model in the Folder
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_introduction_to_word_vectors.html">
   2. Introduction to Word Vectors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_generating_custom_word_vectors.html">
   3. Generating Custom Word Vectors with Gensim
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08_loading_custom_word_vectors.html">
   4. Loading Custom Word Vectors into a spaCy Model
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Solving a Domain-Specific Problem (Holocaust)
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="09_01_cultivating_concentration_camp_dataset.html">
   1. Cultivating Good Datasets for Entities
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2. Cultivating a Corpus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_the_challenges_of_holocaust_ner.html">
   3. The Challenges of Holocaust NER
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11_introduction_to_custom_pipes.html">
   4. Introduction to Custom Pipes
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/09_02_cultivating_corpus.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/09_02_cultivating_corpus.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#key-concepts-in-this-notebook">
   2.1. Key Concepts in this Notebook
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   2.2. Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variety">
   2.3. Variety
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preparing-the-corpus">
   2.4. Preparing the Corpus
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#to-lemmatize-or-not-to-lemmatize">
   2.5. To Lemmatize or Not to Lemmatize
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#segment-the-corpus">
   2.6. Segment the Corpus
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise">
   2.7. Exercise
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#video">
   2.8. Video
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="center-cultivating-a-corpus-center">
<h1><span class="section-number">2. </span><center>Cultivating a Corpus</center><a class="headerlink" href="#center-cultivating-a-corpus-center" title="Permalink to this headline">¶</a></h1>
<center>Dr. W.J.B. Mattingly</center>
<center>Smithsonian Data Science Lab and United States Holocaust Memorial Museum</center>
<center>January 2021</center><div class="section" id="key-concepts-in-this-notebook">
<h2><span class="section-number">2.1. </span>Key Concepts in this Notebook<a class="headerlink" href="#key-concepts-in-this-notebook" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>What to look for in a corpus<br></p></li>
</ol>
</div>
<div class="section" id="introduction">
<h2><span class="section-number">2.2. </span>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>This notebook has a single purpose, to provide the steps I took to cultivate a corpus to run an EntityRuler across to cultivate a large training set. I will use this as a case study for what to look for in a corpus.</p>
</div>
<div class="section" id="variety">
<h2><span class="section-number">2.3. </span>Variety<a class="headerlink" href="#variety" title="Permalink to this headline">¶</a></h2>
<p>The goal of machine learning models is to be able to generalize well on unseen data. The best way to get the model to generalize well on all unseen data (or at least a lot of it), is to give the model texts is a wide variety. Variety is key when training because variety allows for a model to learn different things from different types of texts, which are written in different styles, dialects, contexts, etc. So the first thing you need to consider when putting together a corpus to generate training data from is, variety. So, with that in mind, give the model as much textual data as you can.</p>
</div>
<div class="section" id="preparing-the-corpus">
<h2><span class="section-number">2.4. </span>Preparing the Corpus<a class="headerlink" href="#preparing-the-corpus" title="Permalink to this headline">¶</a></h2>
<p>Once you have compiled documents from various sources that represent a wide range of styles, dialects, and contexts, it’s time to begin preparing that corpus. At this stage, the NLP practitioner must consider <em>how</em> they want to prepare the corpus. How one prepares the corpus radically alters <em>what</em> a model learns.</p>
<p>Let’s consider these few sentences:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sent1</span> <span class="o">=</span> <span class="s2">&quot;My name is William Mattingly.&quot;</span>
<span class="n">sent2</span> <span class="o">=</span> <span class="s2">&quot;My name is William Mattingly&quot;</span>
<span class="n">sent3</span> <span class="o">=</span> <span class="s2">&quot;my name is william mattingly&quot;</span>
<span class="n">sent4</span> <span class="o">=</span> <span class="s2">&quot;MY NAME IS WILLIAM MATTINGLY&quot;</span>
</pre></div>
</div>
</div>
</div>
<p>While each of these sentences is identical to humans, but to a machine they are entirely different. Let me demonstrate what I mean.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sents</span> <span class="o">=</span> <span class="p">[</span><span class="n">sent1</span><span class="p">,</span> <span class="n">sent2</span><span class="p">,</span> <span class="n">sent3</span><span class="p">,</span> <span class="n">sent4</span><span class="p">]</span>

<span class="n">all_words</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sents</span><span class="p">:</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">sent</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="n">all_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

<span class="n">no_duplicates</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">all_words</span><span class="p">))</span>
<span class="n">no_duplicates</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">no_duplicates</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;IS&#39;, &#39;MATTINGLY&#39;, &#39;MY&#39;, &#39;Mattingly&#39;, &#39;Mattingly.&#39;, &#39;My&#39;, &#39;NAME&#39;, &#39;WILLIAM&#39;, &#39;William&#39;, &#39;is&#39;, &#39;mattingly&#39;, &#39;my&#39;, &#39;name&#39;, &#39;william&#39;]
</pre></div>
</div>
</div>
</div>
<p>In the above code, we combined all sentences into a bag of words and then removed duplicates. Notice, however, that despite removing duplicates, each word appears multiple times. This is because the computer does not see “WILLIAM”, “William”, or “william” as the same word. How you clean and prepare your training data will affect how your model understands its world. In other words, if you clean your data and expect your model to be given unclean data, it will not perform well. If you lowercase all your training data and your model encounters capitalized words, it will not perform well.</p>
<p>Cleaning and preparing data are fundamental steps in natural language processing, but when it comes to training, you, as the creator of your model, must understand how those steps will affect the performance of your model and should you take the steps to clean your data in a specific way, you should let your users know the steps you took and how data should be given to your model. You may even want to provide a few functions or classes to your users to help them clean their data and structure it in a way that you did for training.</p>
<p>Let’s take a brief look at how we might achieve this with the code below. It is identical to the code above, except each sentence is lowercased and the period removed before being appended to the all_words list.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sents</span> <span class="o">=</span> <span class="p">[</span><span class="n">sent1</span><span class="p">,</span> <span class="n">sent2</span><span class="p">,</span> <span class="n">sent3</span><span class="p">,</span> <span class="n">sent4</span><span class="p">]</span>

<span class="n">all_words</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sents</span><span class="p">:</span>
    <span class="c1">#New line in which we lowercase and remove the period punctuation.</span>
    <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="n">sent</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">sent</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="n">all_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

<span class="n">no_duplicates</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">all_words</span><span class="p">))</span>
<span class="n">no_duplicates</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">no_duplicates</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>my name is william mattingly
my name is william mattingly
my name is william mattingly
my name is william mattingly
[&#39;is&#39;, &#39;mattingly&#39;, &#39;my&#39;, &#39;name&#39;, &#39;william&#39;]
</pre></div>
</div>
</div>
</div>
<p>By cleaning the text, we’ve succcessfully eliminated. However, in doing this, our model will never encounter data where proper nouns are uppercased. It will never encounter periods, either. In cleaning your data, therefore, be mindful of this.</p>
</div>
<div class="section" id="to-lemmatize-or-not-to-lemmatize">
<h2><span class="section-number">2.5. </span>To Lemmatize or Not to Lemmatize<a class="headerlink" href="#to-lemmatize-or-not-to-lemmatize" title="Permalink to this headline">¶</a></h2>
<p>Next, we must consider the question of lemmatization. We met lemmatization in notebook 01.02, but perhaps it is worth describing it again here. Lemmatization is the process by which we reduce all words to their lemma, or root. There are different libraries that perform lemmatization differently. For our purposes, we will use spaCy. Let’s look at a new example in the sentences below.</p>
<p>In both sentences, we have almost the same meaning except the concept of time and one being entirely uppercase. Lemmatization offers the ability to make these both sentences identical.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>

<span class="n">sent1</span> <span class="o">=</span> <span class="s2">&quot;The ball is his.&quot;</span>
<span class="n">sent2</span> <span class="o">=</span> <span class="s2">&quot;THE BALL WAS HIS.&quot;</span>

<span class="n">sents</span> <span class="o">=</span> <span class="p">[</span><span class="n">sent1</span><span class="p">,</span> <span class="n">sent2</span><span class="p">]</span>

<span class="n">all_words</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sents</span><span class="p">:</span>
    <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span>
    <span class="n">new</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;-&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">token</span><span class="o">.</span><span class="n">lemma_</span><span class="p">:</span>
            <span class="n">new</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">lemma_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
    <span class="n">sent</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">new</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="n">sent</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">sent</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="n">all_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

<span class="n">no_duplicates</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">all_words</span><span class="p">))</span>
<span class="n">no_duplicates</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">no_duplicates</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>the ball be his
the ball be his
[&#39;ball&#39;, &#39;be&#39;, &#39;his&#39;, &#39;the&#39;]
</pre></div>
</div>
</div>
</div>
<p>By performing lemmatization on the sentences, we were able to entirely eliminate the temporal difference between these two texts. This is occasionally a useful step in cleaning textual data for various NLP tasks, and sometimes it is good to introduce this simplified forms to texts, depending on what you want the model to do. Again, by doing this, it means a model will never encounter “was” or any non-lemmatized forms of words. Again, what you give the model, is what the model will learn.</p>
</div>
<div class="section" id="segment-the-corpus">
<h2><span class="section-number">2.6. </span>Segment the Corpus<a class="headerlink" href="#segment-the-corpus" title="Permalink to this headline">¶</a></h2>
<p>A final note before we wrap up this notebook is on segmentation. Text segmentation is the process by which you break up the large corpus (sometimes millions of documents) into managable sizes delinated in a single text file by line breaks (my preference). I find it is always good to think about the size of training data I want to give the model. If I am working with Latin texts, I choose a segment size that is a chapter (usually 4-20 sentences). If I am working with USHMM oral testimonies, I segment the corpus by blocks of question-answer. If I am working with regular textual data, I may segment the text at individual sentences.</p>
<p>Think about the size of the training data you want to give the model. I find for the process of automating a rules-based EntityRuler for generating training data, smaller sizes eliminates the potential for false negatives.</p>
</div>
<div class="section" id="exercise">
<h2><span class="section-number">2.7. </span>Exercise<a class="headerlink" href="#exercise" title="Permalink to this headline">¶</a></h2>
<p>For the exercise in this notebook, find a corpus that meets the requirements I mentioned above and expirement with different methods of cleaning that corpus.</p>
</div>
<div class="section" id="video">
<h2><span class="section-number">2.8. </span>Video<a class="headerlink" href="#video" title="Permalink to this headline">¶</a></h2>
<p>I do not have a video for this notebook, unfortunately.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="09_01_cultivating_concentration_camp_dataset.html" title="previous page"><span class="section-number">1. </span><center>Cultivating Good Datasets for Entities</center></a>
    <a class='right-next' id="next-link" href="10_the_challenges_of_holocaust_ner.html" title="next page"><span class="section-number">3. </span><center>The Challenges of Holocaust NER</center></a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By William Mattingly<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>