
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Custom Named Entity Recognition for Holocaust Documents &#8212; Holocaust NER Lessons</title>
    
  <link rel="stylesheet" href="_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Custom Named Entity Recognition for Holocaust Documents" href="07_generating_custom_word_vectors.html" />
    <link rel="prev" title="Custom Named Entity Recognition for Holocaust Documents" href="05_examining_a_spacy_model.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  
  <h1 class="site-logo" id="site-title">Holocaust NER Lessons</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Overview
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="01_01_introduction_to_ner.html">
   Introduction to Named Entity Recognition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="01_02_introduction_to_spacy.html">
   Introduction to spaCy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_rules_based_ner.html">
   Rules-Based NER with spaCy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_machine_learning_ner.html">
   Machine Learning NER with spaCy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_01_spaCy_Entity_Ruler.html">
   Using SpaCy's EntityRuler
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_02_create_ner_training_set.html">
   Using EntityRuler to Create Training Set
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_03_train_spacy_ner_model.html">
   How to Train spaCy NER Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_examining_a_spacy_model.html">
   Examining a spaCy Model in the Folder
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Introduction to Word Vectors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_generating_custom_word_vectors.html">
   Generating Custom Word Vectors with Gensim
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08_loading_custom_word_vectors.html">
   Loading Custom Word Vectors into a spaCy Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_01_cultivating_concentration_camp_dataset.html">
   Cultivating Good Datasets for Entities
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/06_introduction_to_word_vectors.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/06_introduction_to_word_vectors.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#center-part-06-center-br-center-introduction-to-word-vectors-center">
   <center>
    Part 06:
   </center>
   <br/>
   <center>
    Introduction to Word Vectors
   </center>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#key-concepts-in-this-notebook">
     Key Concepts in this Notebook
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#before-you-start-reading">
     Before you Start Reading!
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-need-to-represent-text-numerically">
     The Need to Represent Text Numerically
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-use-word-vectors">
     Why use Word Vectors?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-do-word-vectors-look-like">
     What do Word Vectors Look Like?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Why use Word Vectors?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#video">
     Video
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="center-custom-named-entity-recognition-for-holocaust-documents-center">
<h1><center> Custom Named Entity Recognition for Holocaust Documents</center><a class="headerlink" href="#center-custom-named-entity-recognition-for-holocaust-documents-center" title="Permalink to this headline">¶</a></h1>
<div class="section" id="center-part-06-center-br-center-introduction-to-word-vectors-center">
<h2><center>Part 06:</center><br><center>Introduction to Word Vectors</center><a class="headerlink" href="#center-part-06-center-br-center-introduction-to-word-vectors-center" title="Permalink to this headline">¶</a></h2>
<center>Dr. W.J.B. Mattingly</center>
<center>Smithsonian Data Science Lab and United States Holocaust Memorial Museum</center>
<center>January 2021</center><div class="section" id="key-concepts-in-this-notebook">
<h3>Key Concepts in this Notebook<a class="headerlink" href="#key-concepts-in-this-notebook" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>Word Vectors (Word Embeddings)<br></p></li>
<li><p>Matrix<br></p></li>
<li><p>Bigrams and Trigrams<br></p></li>
<li><p>Bag of Words<br></p></li>
</ol>
</div>
<div class="section" id="before-you-start-reading">
<h3>Before you Start Reading!<a class="headerlink" href="#before-you-start-reading" title="Permalink to this headline">¶</a></h3>
<p>Before you star reading, I want you to do one thing. Think about the word concentation camp. Think about what concepts it evokes. Think about the first few words that come to mind. Write down a few proper nouns that come to mind as well.</p>
</div>
<div class="section" id="the-need-to-represent-text-numerically">
<h3>The Need to Represent Text Numerically<a class="headerlink" href="#the-need-to-represent-text-numerically" title="Permalink to this headline">¶</a></h3>
<p>Word vectors, or word embeddings, are numerical representations of words in multidimensional space through matrices. The purpose of the word vector is to get a computer system to understand a word. Computers cannot understand text efficiently. They can, however, process numbers quickly and well. For this reason, it is important to convert a word into a number.</p>
<p>Initial methods for creating word vectors in a pipeline take all words in a corpus and convert them into a single, unique number. These words are then stored in a dictionary that would look like this: {“the”: 1, “a”, 2} etc. This is known as a <strong>bag of words</strong>. This approach to representing words numerically, however, only allow a computer to understand words numerically to identify unique words. It does not, however, allow a computer to understand <em>meaning</em>.</p>
<p>Imagine this scenario:</p>
<p>Tom loves to eat chocolate.</p>
<p>Tom likes to eat chocolate.</p>
<p>These sentences represented as a numerical array (list) would look like this:</p>
<p>1, 2, 3, 4, 5</p>
<p>1, 6, 3, 4, 5</p>
<p>As we can see, as humans both sentences are nearly identical. The only difference is the degree to which Tom appreciates eating chocolate. If we examine the numbers, however, these two sentences seem quite close, but their semantical meaning is impossible to know for certain. How similar is 2 to 6? The number 6 could represent “hates” as much as it could represent “likes”. This is where word vectors come in.</p>
<p>Word vectors take these one dimensional bag of words and gives them multidimensional meaning by representing them in higher dimensional space, noted above. This is achieved through machine learning and can be easily achieved via Python libraries, such as Gensim, which we will explore more closely in the next notebook.</p>
</div>
<div class="section" id="why-use-word-vectors">
<h3>Why use Word Vectors?<a class="headerlink" href="#why-use-word-vectors" title="Permalink to this headline">¶</a></h3>
<p>The goal of word vectors is to achieve numerical understanding of language so that a computer can perform more complex tasks on that corpus. Let’s consider the example above. How do we get a computer to understand 2 and 6 are synonyms or mean something similar? One option you might be thinking is to simply give the computer a synonym dictionary. It can look up synonyms and then know what words mean. This approach, on the surface, makes perfect sense, but let’s explore that option and see why it cannot possibly work.</p>
<p>For the example below, we will be using the Python library PyDictionary which allows us to look up definitions and synonyms of words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">PyDictionary</span> <span class="kn">import</span> <span class="n">PyDictionary</span>

<span class="n">dictionary</span><span class="o">=</span><span class="n">PyDictionary</span><span class="p">()</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Tom loves to eat chocolate&quot;</span>

<span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="n">syns</span> <span class="o">=</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">synonym</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">syns</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tom: [&#39;Felis domesticus&#39;, &#39;tomcat&#39;, &#39;domestic cat&#39;, &#39;gib&#39;, &#39;house cat&#39;]

loves: [&#39;amorousness&#39;, &#39;caring&#39;, &#39;lovingness&#39;, &#39;agape&#39;, &#39;adoration&#39;]

to: [&#39;digitizer&#39;, &#39;data converter&#39;, &#39;digitiser&#39;, &#39;analog-digital converter&#39;]

eat: [&#39;consume&#39;, &#39;garbage down&#39;, &#39;eat up&#39;, &#39;gluttonize&#39;, &#39;take in&#39;]

chocolate: [&#39;drinking chocolate&#39;, &#39;drink&#39;, &#39;drinkable&#39;, &#39;potable&#39;, &#39;beverage&#39;]
</pre></div>
</div>
</div>
</div>
<p>Even with the simple sentence, the results are comically bad. Why? The reason is because synonym substitution, a common method of data augmentation, does not take into account syntactical differences of synonyms. I do not believe anyone would think “Felis domesticus”, the Latin name of the common house cat, would be an adequite substitution for the name Tom. Nor is “garbage down” a really proper synonym for eat.</p>
<p>Perhaps, then we could use synonyms to find words that have cross-terms, or terms that appear in both synonym sets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">PyDictionary</span> <span class="kn">import</span> <span class="n">PyDictionary</span>

<span class="n">dictionary</span><span class="o">=</span><span class="n">PyDictionary</span><span class="p">()</span>

<span class="n">words</span>  <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;like&quot;</span><span class="p">,</span> <span class="s2">&quot;love&quot;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="n">syns</span> <span class="o">=</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">synonym</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">syns</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>like: [&#39;love&#39;, &#39;prefer&#39;, &#39;enjoy&#39;, &#39;cotton&#39;, &#39;care for&#39;]

love: [&#39;amorousness&#39;, &#39;caring&#39;, &#39;lovingness&#39;, &#39;agape&#39;, &#39;adoration&#39;]
</pre></div>
</div>
</div>
</div>
<p>This, as we can see, has some potential to work, but again it is not entirely reliable and to work with such a list would be computationally expensive. For both of these reasons, word vectors are prefered. The reason? Because they are formed by the computer on corpora for a specific task. Further, they are numerical in nature (not a dictionary of words), meaning the computer can process them more quickly.</p>
</div>
<div class="section" id="what-do-word-vectors-look-like">
<h3>What do Word Vectors Look Like?<a class="headerlink" href="#what-do-word-vectors-look-like" title="Permalink to this headline">¶</a></h3>
<p>Word vectors have a preset number of dimensions. These dimensions are honed via machine learned. Models take into account word frequency alongside words across a corpus and the appearance of other words in similar contexts. This allows for the the computer to determin the syntactical similarity of words numerically. It then needs to represent these relationships numerically. It does this through the vector, or a matrix of matrices. To represent these more concisely, models flatten a matrix to a float (decimal number). The number of dimensions represent the number of floats in the matrix.</p>
<p>Below is a pretrained model’s output of word vectors for Holocaust documents. This is how the word “know” looks in vectors:</p>
<p>know -0.19911548 -0.27387282 0.04241912 -0.58703226 0.16149549 -0.08585547 -0.10403373 -0.112367705 -0.28902963 -0.42949626 0.051096343 -0.04708015 -0.051914077 -0.010533272 -0.23334776 0.031974062 -0.015784053 -0.21945408 0.07359381 0.04936823 -0.15373217 -0.18460844 -0.055799782 -0.057939123 0.14816307 -0.46049833 0.16128318 0.190906 -0.29180774 -0.08877125 0.23563664 -0.036557104 -0.23812544 0.21938106 -0.2781296 0.5112853 0.049084224 0.14876273 0.20611146 -0.04535578 -0.35051352 -0.26381743 0.20824358 0.29732847 -0.013382204 -0.19970295 -0.34890386 -0.16214448 -0.23497184 0.1656344 0.15815939 0.012848561 -0.22887675 -0.21618247 0.13367777 0.1028471 0.25068823 -0.13625076 -0.11771541 0.4857257 0.102198474 0.06380113 -0.22328818 -0.05281015 0.0059655504 0.095453635 0.39693353 -0.066147 -0.1920163 0.5153346 0.24972811 -0.0076305643 -0.05530072 -0.24668717 -0.074051596 0.29288396 -0.0849124 0.37786478 0.2398532 -0.10374063 0.5445305 -0.41955113 0.39866814 -0.23992492 -0.15373677 0.34488577 -0.07166888 -0.48001364 0.0660652 0.061260436 0.32197484 -0.12741785 0.024006622 -0.07915035 -0.04467735 -0.2387938 -0.07527494 0.07079664 0.074456714 0.17877163 -0.002122373 -0.16164272 0.12381973 -0.5908519 0.5827627 -0.38076186 0.095964395 0.020342976 -0.5244792 0.24467848 -0.12481717 0.2869162 -0.34473857 -0.19579992 -0.18069582 0.015281798 -0.18330036 -0.08794056 0.015334953 -0.5609912 0.17393902 0.04283724 -0.07696586 0.2040299 0.34686008 0.31219167 0.14669564 -0.26249585 -0.42771882 0.5381632 -0.123247474 -0.29142144 -0.29963812 -0.32800657 -0.10684048 -0.08594837 0.19670585 0.13474767 0.18349588 -0.4734125 0.15554792 -0.21062694 -0.14191462 -0.12800062 0.2053445 -0.05258381 0.10878109 0.56381494 0.22724482 -0.17778987 -0.061046753 0.10789692 -0.015310492 0.16563527 -0.31812978 -0.1478078 0.4323269 -0.2543924 -0.25956103 0.38653126 0.5080214 -0.18796602 -0.10318089 0.023921987 -0.14618908 0.22923793 0.37690258 0.13323267 -0.34325415 -0.048353776 -0.30283198 -0.2839813 -0.2627738 -0.07422618 -0.31940162 0.38072023 0.56700015 -0.023362642 -0.3786432 0.084006436 0.0729958 0.09483505 -0.2665334 0.12699558 -0.37927982 -0.39073908 0.0063185897 -0.34464878 -0.24011964 0.09303968 -0.15488827 -0.018486138 0.3560308 -0.26005003 0.089302294 0.116130605 0.07684872 -0.085253105 -0.28178927 -0.17346472 -0.20008522 0.004347025 0.34192443 0.017453942 0.06926512 -0.15926014 -0.018554512 0.18478563 -0.040194467 0.38450953 0.4104423 -0.016453728 0.013374495 -0.011256633 0.09106963 0.20074937 0.17310189 -0.12467103 0.16330549 -0.0009963055 0.12181527 -0.05295286 -0.0059491103 -0.04697837 0.38616535 -0.21074814 -0.32234505 0.47269863 0.27924335 0.13548143 -0.2677968 0.03536313 0.3248672 0.2062973 0.29093853 0.1844036 -0.43359983 0.025519002 -0.06319317 -0.2427806 -0.22732906 0.08803728 -0.041860744 -0.151291 0.3400458 -0.29143015 0.25334117 0.06265491 0.26399022 -0.20121849 0.22156847 -0.50599706 0.069224015 0.52325517 -0.34115726 -0.105219565 -0.37346402 -0.02126528 0.09619415 0.017722093 -0.3621799 -0.109912336 0.021542747 -0.13361925 0.2087667 -0.08780184 0.09494446 -0.25047818 -0.07924239 0.21750642 0.2621652 -0.52888566 0.081884995 -0.20485449 0.18029206 -0.5623824 -0.03897387 0.3213515 0.057455678 -0.26524526 0.14741589 0.1257589 0.04708992 0.026751317 -0.014696863 -0.11038961 0.004459205 -0.01394376 0.091146186 -0.15486309 0.20662159 -0.0987916 -0.07740813 0.009704136 0.28866896 0.3916269 0.35061485 0.31678385 0.43233085 0.44510433</p>
<p>For these vectors, I used the industry-standard of 300 dimensions. We see each of these dimensions represented by each of the floats, separated by whitespace. As the model passes over the corpus it is being trained on, it hones these numbers and changes them for each word. Over multiple epochs, or generations, it gains a clearer sense of the similarity of words, or at least words that are used in similar contexts.</p>
</div>
<div class="section" id="id1">
<h3>Why use Word Vectors?<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Once a word vector model is trained, we can do similarity matches very quickly and very reliably. At the start of the notebook, I asked you to consider the word concentration camp. Let’s now use these word vectors to find the 10 most similar words to concentration camp.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;extermination_camp&#39;</span><span class="p">,</span> <span class="mf">0.5768706798553467</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;camp&#39;</span><span class="p">,</span> <span class="mf">0.5369070172309875</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Flossenbiirg&#39;</span><span class="p">,</span> <span class="mf">0.5099129676818848</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Sachsenhausen&#39;</span><span class="p">,</span> <span class="mf">0.5068483948707581</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Auschwitz&#39;</span><span class="p">,</span> <span class="mf">0.48929861187934875</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Dachau&#39;</span><span class="p">,</span> <span class="mf">0.4765608310699463</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;concen&#39;</span><span class="p">,</span> <span class="mf">0.4753464460372925</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Majdanek&#39;</span><span class="p">,</span> <span class="mf">0.4740387797355652</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Sered&#39;</span><span class="p">,</span> <span class="mf">0.47086501121520996</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Buchenwald&#39;</span><span class="p">,</span> <span class="mf">0.4692303538322449</span><span class="p">)</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>These are the items that are most similar to concentration camp in our word vectors. The tuple has two indices. Index 0 is the word and index 1 is the similarity, represented as a float.</p>
<p>Exterimination camp is not a direct synonym, as it has a distinction in what happened to prisoners, i.e. execution, however, these are very similar. Seeing this as the most similar word is a sign that the word vectors are well-aligned. Camp is expected as it is a singular word that has similar meaning in context to contentration camp. The remainder of this list are proper nouns, all of which were concentration camps with one exception: “concen”. This is clearly a result of poor cleaning. Concen is not a word, rather a type of concen-tration, most likely. The fact that this is here is also a good sign that our word vectors have aligned well enough to have typos in near vector space.</p>
<p>Let’s do something similar with Auschwitz.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;Auschwitz_Birkenau&#39;</span><span class="p">,</span> <span class="mf">0.6649479866027832</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Birkenau&#39;</span><span class="p">,</span> <span class="mf">0.5385118126869202</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;subcamp&#39;</span><span class="p">,</span> <span class="mf">0.5343026518821716</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;camp&#39;</span><span class="p">,</span> <span class="mf">0.533636748790741</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;III&#39;</span><span class="p">,</span> <span class="mf">0.5323576927185059</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;stutthof&#39;</span><span class="p">,</span> <span class="mf">0.518073320388794</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Ravensbriick&#39;</span><span class="p">,</span> <span class="mf">0.5084848403930664</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Berlitzer&#39;</span><span class="p">,</span> <span class="mf">0.5083401203155518</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Malchow&#39;</span><span class="p">,</span> <span class="mf">0.5051567554473877</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Oswiecim&#39;</span><span class="p">,</span> <span class="mf">0.5016494393348694</span><span class="p">)</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>As we can see, the words closest to Auchwitz are places assocaited with Auschwitz, such as Birkenau, subcamps (of which Auschwitz had many), other concentration camps (such as Ravensbriick), and the location of the Auschwitz memorial, Oswiecim.</p>
<p>In other words, we have words closely associated with Auschwitz in particular.</p>
<p>In the next video, we will be looking closely at how to generate word vectors via the library Gensim. To get a better sense of word vectors, please watch the video below.</p>
</div>
<div class="section" id="video">
<h3>Video<a class="headerlink" href="#video" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%html</span>
<span class="p">&lt;</span><span class="nt">div</span> <span class="na">align</span><span class="o">=</span><span class="s">&quot;center&quot;</span><span class="p">&gt;</span>
<span class="p">&lt;</span><span class="nt">iframe</span> <span class="na">width</span><span class="o">=</span><span class="s">&quot;560&quot;</span> <span class="na">height</span><span class="o">=</span><span class="s">&quot;315&quot;</span> <span class="na">src</span><span class="o">=</span><span class="s">&quot;https://www.youtube.com/embed/eZJm7PisZvk&quot;</span> <span class="na">frameborder</span><span class="o">=</span><span class="s">&quot;0&quot;</span> <span class="na">allow</span><span class="o">=</span><span class="s">&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot;</span> <span class="na">allowfullscreen</span><span class="p">&gt;&lt;/</span><span class="nt">iframe</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/eZJm7PisZvk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
</div></div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="05_examining_a_spacy_model.html" title="previous page"><center> Custom Named Entity Recognition for Holocaust Documents</center></a>
    <a class='right-next' id="next-link" href="07_generating_custom_word_vectors.html" title="next page"><center> Custom Named Entity Recognition for Holocaust Documents</center></a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By William Mattingly<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>